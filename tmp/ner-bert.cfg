# Training hyper-parameters and additional features.
[training]
# Whether to train on sequences with 'gold standard' sentence boundaries
# and tokens. If you set this to true, take care to ensure your run-time
# data is passed in sentence-by-sentence via some prior preprocessing.
gold_preproc = false
# Limitations on training document length or number of examples.
max_length = 500
limit = 0
# Data augmentation
orth_variant_level = 0.0
dropout = 0.1
# Controls early-stopping. 0 or -1 mean unlimited.
patience = 6000
max_epochs = 0
# There's a bug in the spaCy v3.0.0a0 max_steps that means we need
# to multiply by accumulate_gradient. It's fixed, but for now...
max_steps = 100000
eval_frequency = 400
# Other settings
seed = 0
width = 768
use_pytorch_for_gpu_memory = true
# Control how scores are printed and checkpoints are evaluated.
scores = ["speed", "tags_acc", "uas", "las", "ents_f"]
score_weights = {"las": 0.4, "ents_f": 0.4, "tags_acc": 0.2}
# These settings are invalid for the transformer models.
init_tok2vec = null
vectors = null
discard_oversize = true
evaluation_batch_size = 256
batch_by = "words"
batch_size = 1000
accumulate_gradient = 1

[training.optimizer]
@optimizers = "Adam.v1"
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 1e-8

[training.optimizer.learn_rate]
@schedules = "warmup_linear.v1"
warmup_steps = 250
total_steps = 20000
initial_rate = 5e-5

[nlp]
lang = "en"
vectors = ${training:vectors}

[nlp.pipeline]

# Define the pipeline. The train-from-config command looks up the factories
# and assembles the NLP object. You can register more factories using the
# @component decorator.

[nlp.pipeline.transformer]
# This component holds the actual transformer (wrapped via pytorch). The 
# transformer is run once and then the predictions are used by the other
# components. Weights are updated from multi-task gradients.
factory = "transformer"
max_batch_items = 4096

[nlp.pipeline.ner]
factory = "ner"


# This loads the Huggingface Transformers model. The transformer is applied
# to a batch of Doc objects, which are preprocessed into Span objects to support
# longer documents.
[nlp.pipeline.transformer.model]
@architectures = "spacy-transformers.TransformerModel.v1"
name = "roberta-base"
tokenizer_config = {"use_fast": true}

[nlp.pipeline.transformer.model.get_spans]
# You can set a custom strategy for preparing spans from the batch, e.g. you
# can predict over sentences. Here we predict over the whole document.
@span_getters = "strided_spans.v1"
window = 128
stride = 128

[nlp.pipeline.ner.model]
@architectures = "spacy.TransitionBasedParser.v1"
nr_feature_tokens = 3
hidden_width = 64
maxout_pieces = 2
use_upper = false

# These "listener" layers are connected to the transformer pipeline component
# in order to achieve multi-task learning across the pipeline.
# They rely on the transformer to predict over the batch and cache the result
# and callback. The gradient for the transformers will be accumulated by
# the listeners, and then the last listener will call the backprop callback.

[nlp.pipeline.ner.model.tok2vec]
@architectures = "spacy-transformers.Tok2VecListener.v1"
grad_factor = 1.0

# These pooling layers control how the token vectors are calculated from
# the word pieces. The reduce_mean layer averages the wordpieces, so if you
# have one token aligned to multiple wordpieces (as is expected), the token's
# vector will be the average of the wordpieces. The most obvious alternative
# is reduce_last.v1, which would just use the last wordpiece. You could also
# try reduce_first, reduce_sum or even reduce_max.

[nlp.pipeline.ner.model.tok2vec.pooling]
@layers = "reduce_mean.v1"
